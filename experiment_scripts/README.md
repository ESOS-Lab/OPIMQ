# ﻿README for OPIMQ Artifacts

## Introduction
This README provides instructions for reproducing the results presented in the paper "OPIMQ: Order-Preserving IO Stack for Multi-Queue Block Device." 
The artifact package contains the necessary code, benchmarks, and scripts to evaluate OPIMQ and verify its functionality.
The source code for both OPIMQ and OPFTL is publicly available on GitHub. These repositories include the implementation details and instructions on how to apply and use OPIMQ and OPFTL.

## Getting Started Instructions 
### Prerequisites
* Local machine
* CentOS 7.9, Linux 5.18.18 
* **OPIMQ kernel** and **original kernel** are installed and properly configured on your system.
* SSH client installed on your local machine.
* Access credentials for the experiment server.

### Quick Test
1. Switch the your account with root.
su
2. We recommend using tmux.
tmux
3. Run the following script to validate the setup:
cd ./QUICK_TEST/
./quick_test.sh

The key idea of OPIMQ is to eliminate the overhead of the transfer-and-flush operation by assigning the following identifiers to each write request that requires order preservation: Stream ID, Epoch ID, and Barrier Flag. 

The `quick_test.sh` script is designed to validate the core functionalities of OPIMQ. This script formats the target NVMe device, the Samsung 980 PRO NVMe SSD, with OPEXT4, mounts it to the `/mnt` directory, and verifies the two stream id, spoch id pairs, i.e <major stream id, major epoch id>, <minor stream id, minor epoch id>  and barrier flag valus eassigned to each I/O request by OPIMQ using Linux ftrace. 

quick_test.sh includes the following steps:
1. OPEXT4 Filesystem Initialization and Mounting
   * The NVMe device is formatted with OPEXT4 and mounted to /mnt.
2. 4KB Write + fsync Test
   * Performs a 4KB write operation followed by an fsync using FIO. The FIO script used for the experiment is located at ./fio_quick_test.
      * Write size: 4KB, File size: 16KB
      * 4KB random write followed by fsync().
3. ftrace Logging
   * During the creation of NVMe commands for I/O operations, two pairs of Stream IDs and Epoch IDs, along with the Barrier Flag values, are stored in the kernel buffer. After the experiment concludes, the tracing logs are saved in the ./LOGS directory.
Expected Output
The file ./LOGS/parsed_data contains the parsed output from the raw trace data.
This parsed output organizes the trace data into a tab-separated format, making it easier to analyze and visualize.
For example, a portion of the parsed data might look like this:

| Timestamp   | PName  | PID  | [sid1, eid1] | [sid2, eid2] | bflag | qid | optype          |
|-------------|--------|------|--------------|--------------|-------|-----|-----------------|
| 153.34838   | fio    | 2694 | [0, 0]       | [0, 0]       | 0     | 20  | nvme_cmd_read   |
| 153.465645  | fio    | 2736 | [2736, 0]    | [2688, 0]    | 1     | 1   | nvme_cmd_write  |
| 153.465664  | jbd2   | 2688 | [2688, 1]    | [0, 0]       | 0     | 2   | nvme_cmd_write  |
| 153.466377  | jbd2   | 2688 | [2688, 1]    | [0, 0]       | 0     | 2   | nvme_cmd_write  |
| 153.46639   | jbd2   | 2688 | [2688, 2]    | [0, 0]       | 1     | 2   | nvme_cmd_write  |
| 153.4664    | kworker| 784  | [2688, 3]    | [0, 0]       | 1     | 4   | nvme_cmd_flush  |


#### Column Descriptions
* Timestamp: The time (in seconds) when the event (create nvme command structure) occurred.
* PName: The process name (e.g., fio, jbd2, kworker).
* PID: The process ID.
* <sid1, eid1>: The major stream and epoch id pair. This is set for all order-preserving writes.
* <sid2, eid2>: The minor stream and epoch id pair, only set for dual-stream writes (e.g., dirty page write requests generated by an fsync call).
* bflag: barrier flag; a value of 1 indicates that a cache barrier command is placed after this I/O request.
* qid: The ID of the NVMe queue where the I/O request is registered.
* optype: The type of NVMe operation (e.g., read, write).
* Notes: The role of each write request (e.g., D for dirty page write, JD1,JD2 for journal descriptor, JC for journal commit).

#### Ordering Dependency Preservation
Now, we will explain how the major stream, epoch ID, minor stream, epoch ID, and barrier flag assigned by OPIMQ ensure the storage orderer, as observed through ftrace. 
To ensure file system consistency in the EXT4 file system's ordered mode journaling (default mode), the dirty page write request (D) generated by the thread calling fsync() (in this case, FIO application thread) and the journal descriptor write request (JD) generated by the JBD2 thread, the EXT4 file system daemon, must be durably recorded on disk before the journal commit block (JC). This ordering can be represented as:
{D, JD} → {JC}
1. OPIMQ Behavior 
   1. Stream Assignment:
      * OPIMQ assigns the major stream ID to the process generating the request (e.g., 2736 for FIO and 2688 for JBD2).
      * For dual-stream writes, the minor stream ID is assigned to the dependent thread (e.g., 2688 for JBD2 in D).
   2. Epoch Management:
      * Each stream maintains an epoch counter, starting at 0 and incrementing after dispatching an order-preserving request (bflag = 1).
   3. Dual Stream Write: 
      * OPIMQ proposes a dual-stream write technique to ensure write-order dependencies between different threads (in this case, D generated by FIO and JC generated by JBD2). OPIMQ converts the preceding write, D, into a dual-stream write. A write request configured as a dual-stream write reflects the information of the stream (JBD2) with which it has order dependencies, using the minor stream ID and minor epoch ID.
         * D is configured as a dual-stream write, with the minor stream ID set to JBD2's process ID, 2688, and the minor epoch ID set to JBD2's epoch counter value, 0 (initial value).
      * The minor stream ID is set to the stream ID of JBD2, and the minor epoch ID is assigned the epoch counter value of JBD2. If the barrier flag is set for a dual-stream write, OPIMQ increments both the major stream and minor stream epoch counters by 1 after dispatching the write request.
      * For I/O requests that are not dual-stream writes, both the minor stream ID and minor epoch ID are set to 0. In the case of JD1, JD2, and JC, these values are 0.

2. Example Analysis (Based on the Table)
* FIO thread
The FIO thread generates one read request and one write request:
   * Read Request: This is the I/O generated for the initialization of the FIO benchmark. For read operations, since they are not order-preserving I/O, the major stream, epoch, minor stream, and epoch ID values are all set to 0. It can be confirmed that they are correctly set.
   * Write Request (D): 
The second line represents a dirty page write request generated by an fsync() call.
      * Major Stream ID (sid1): The process ID of the FIO thread (2736) is assigned as the major stream id.
      * Epoch ID (eid1): The epoch ID starts at 0 for the FIO process.
      * Minor Stream (sid2, eid2): Assigned to the JBD2 thread (PID 2688, epoch 0), ensuring the dirty page depends on the journal.
      * bflag: Set to 1 to ensure the order in which it is recorded before the journal commit block. A cache barrier is positioned after D.
      * JBD2 thread
      * The JBD2 thread (PID 2688) generates:
      * Journal Descriptor Writes (JD1, JD2):
      * Major Stream ID (sid1): Both descriptor writes are assigned the major stream ID 2688 and the epoch ID 1, matching the updated epoch counter after D is dispatched.
      * The barrier flag for JD2 is set to 1. OPEXT4 sets the barrier flag to 1 for the journal descriptor block write request that is generated last. This ensures that the journal descriptor blocks are durably stored on disk before the journal commit block that will be generated afterward.
      * Journal Commit Block (JC):
      * Major Stream ID (sid1): assigned by the process id of JBD2 (2688)
      * Major Epoch ID (eid1): assigned by the epoch counter of JBD2. When JC is generated, JBD2's epoch counter value is 2. This is because the counter incremented from 0 to 1 after dispatching D and then from 1 to 2 after dispatching JC.
      * Bflag : 1 
      * The barrier flag is set to 1, ensuring durability.
      * Kworker thread
      * The kworker thread (PID ) generates: 
      * Flush request : 
      * Major stream id (sid1): 2688 (PID of JBD2)
      * Major stream id (eid 1): 3 (epoch counter value of JBD2)
      * Bflag : 1 
      * For the next write request generated by the FIO thread (2736) due to an fsync() call, the barrier flag is set to 1 to ensure order preservation.
      *       * When an fsync is invoked, the kworker thread dispatches a flush request. To ensure durability during fsync, OPIMQ assigns a major stream ID and a major epoch ID to the flush request. The epoch ID of the flush command is greater than the epoch IDs of the file's dirty pages, journal logs, and the journal commit block in the JBD stream. OPFTL guarantees the durability of fsync() by ensuring that, when a flush command is issued with stream and epoch IDs, it completes only after all previous epochs in that stream have been persisted.


3. Persistence Ordering in OPIMQ & OPFTL (Based on the Table)
D (Dirty Page Write) Persists Before JC (D->JC):
      	<1> Dual-Stream Write Enforcement:
      * The minor stream ID ([2688, 0]) in D ensures a dependency on the JBD2 thread's initial state (epoch 0). OPFTL recognizes a write request as a dual-stream 	write if its minor stream ID is not 0. For dual-stream writes, the mapping table is updated only when the data from preceding epochs in both streams has 	been fully and persistently written. D is assigned a smaller minor epoch ID than JC. Therefore, D is always persistently written before JC.
   
        <2> Epoch Counter Synchronization:
      * After D is dispatched, the major stream epoch counter for FIO (2736) and the minor stream epoch counter for JBD2 (2688) both increment to 1. 
      * D is always assigned a lower epoch ID than JC in the minor stream (JBD2) due to the configured barrier flag.
	In the case of dual-stream writes, OPFTL ensures persistence order in the minor stream based on ascending epoch IDs.
	Therefore, D is always persisted before JC, which is generated by JBD2.
	JD1 and JD2 Persist Before JC ({JD1,JD2} -> JC): 
	The OPFTL of the SSD recognizes a write request as an order-preserving write if its major stream ID is not 0. It ensures that the mapping table is updated 	in ascending order of the major epoch ID within the respective major stream.
	The major epoch ID of JC is greater than that of JD1 and JD2. In this case, OPFTL delays updating JC's mapping information in the table until all data from 	JD1 and JD2 is written to the flash chips and their mapping information is fully updated in the table. Since the major epoch ID of JC is greater than that 	of JD1 and JD2, it is guaranteed that JC is persistently written after JD1 and JD2.


## Detailed Instructions
We have organized the experiments in the paper that demonstrate the key characteristics of OPIMQ and support the main claims of the paper.
Package Contents
The following directories and files are available:
./QUICK_TEST/ : Containing quick test script for performing the getting started instructions        
./scripts/ : The script path for performing the experiments in the Detailed Instructions.
         * CONTAINER_THROGHPUT/ 
         * EPOCH_PIN/ 
         * FSYNC_LATENCY/ 

### How to change the kernel
Since there are experiments comparing OPIMQ with other order-preserving studies, we will explain how to change and execute kernels on the experimental server we provide.
If you want to compile manually, please move to the corresponding kernel source code directory and run ./recompile.sh (Please refer to the Package Contents).
If any of the steps fail, please refer to the Troubleshooting section.
1. fsync() Latency Comparison
         * Overview
OPIMQ is a multi-queue order-preserving I/O stack designed to optimize fsync operations. It is crucial to demonstrate how OPIMQ reduces fsync latency compared to the traditional Linux I/O stack and the EXT4 file system. The following script reproduces the experiment corresponding to Figure 11 in Section 7.2 of the paper. OPIMQ reduces latency by ~67% compared to EXT4. We run a simple 4KB random write + fsync workload using the FIO benchmark and check the fsync latency through ftrace logs.
         1. Please run the following scripts in OPIMQ kernel and Basline kernel and compare the two outputs.
         1. OPEXT4 + OPIMQ kernel 
uname -r // The output should be 5.18.18-opext4+opimq
cd ./FSYNC_LATENCY
./run.sh
Enter the experiment environment: OPEXT4 (O) or BASELINE (B) //Enter “O”
	cat ./LOGS/OPEXT4/output // Check the output.
	         2. Vanilla Kernel (EXT4)
./kernel.sh 
// Type “B” and enter


sudo reboot


uname -r       // The output should be 5.18.18+original
sudo su
cd ./FSYNC_LATENCY
./run.sh
Enter the experiment environment: OPEXT4 (O) or BASELINE (B) //Enter “B”
cat ./LOGS/EXT4/output // Check the output.
	         3. After (2) and (3) 
./compare.sh displays the results of both EXT4 and OPEXT4 and shows the reduction ratio of fsync latency. There may be up to a 10% margin of error during execution.
cd ./FSYNC_LATENCY
./compare.sh
	         * Expected Output
After running ./compare.sh, the results will appear as follows:
There may be up to a 10% margin of error during execution.
  

         * Analysis: OPEXT4 demonstrates approximately 66.59% lower latency compared to EXT4.
This reduction is achieved by removing the FUA flag from journal commit blocks in OPEXT4. In this experiment, TFUA is 4.19 milliseconds, which accounts for 69% of the total latency of fsync() in EXT4. OPIMQ, on the other hand, does not dispatch the FUA write request, so TFUA is zero.
         * The raw data will be stored, for OPEXT4, in LOGS/OPEXT4 and for EXT4, in LOGS/EXT4. In each directory, file trace represent the raw ftrace data, and output show the parsed results.
How is the latency value obtained?
When executing the run.sh script, logs are collected via ftrace during the following four kernel events:
            * ext4_sync_file_enter(); // fsync starts
            * nvme_setup_cmd(); // dispatch I/O request
            * nvme_sq(); // complete I/O request by the device
            * ext4_sync_file_exit(); // fsync returns
The entire single fsync() latency is the time difference between the ext4_sync_file_exit and ext4_sync_file_enter calls.
THost indicates the time spent on the host (e.g., context switching, generating I/O requests).
TDMA represents the time spent on DMA transfers at the device.
TFlush denotes the latency for processing flush commands on the device.
TFUA measures the time required to process write requests with the FUA flag (e.g., journal commit block writes in EXT4) at the device.
We modified the kernel to calculate the fsync latency and log it in the ftrace event log each time the ext4_sync_file_exit event is triggered.
TDMA + THost is calculated as the timestamp difference between the nvme_sq and nvme_setup_cmd events for write requests (nvme_cmd_write). We implemented a mechanism within the kernel to calculate the latency and store the computed latency in the ftrace log at the time the nvme_sq event is invoked (iolat attribute of nvme_sq event.).
TFlush is computed as the timestamp difference between the nvme_sq and nvme_setup_cmd events for flush commands (nvme_cmd_flush). 
TFUA is determined as the timestamp difference between the nvme_sq and nvme_setup_cmd events for write commands with the FUA flag. In Linux kernels from the 5.x series, when the REQ_FUA flag is set, the kernel is designed such that the kworker dispatches the corresponding write request. From the ftrace logs after the experiment, the last nvme_cmd_write request generated by the kworker thread is the write request with the FUA flag set (journal commit block).
            2. Epoch Pinning Analysis

               * Overview
“Epoch Pinning” is a technique that ensures write requests belonging to the same epoch are always placed in the same I/O queue (Section 4.5 of the paper).
No matter whether the thread is migrated to different cores, Epoch Pinning guarantees that write requests within the same epoch are placed in the same request queue.


This technique plays a crucial role in preventing epoch splits even when a thread is migrated while generating write requests within an epoch, thereby ensuring intra-stream order.


We demonstrate through experiments that the Epoch Pinning mechanism does not introduce overhead to workload throughput (Figure 12 of the paper).


(Please Note 1) We noticed that the captions for (b) dbench and (c) sysbench in Figure 12 are swapped. We apologize for the confusion. The correct captions are (b) sysbench and (c) dbench.


(Please Note 2) We also replaced the device used in the experiments presented in the paper due to a failure of the original device. While the replacement is the same model (Samsung 980 Pro NVMe SSD), it has a capacity of 1TB. Consequently, the absolute throughput performance for Filebench varmail differs from the data reported in the paper. Varmail is a workload that invokes fsync significantly more frequently than dbench or sysbench, which accounts for the discrepancy in results specifically for varmail. However, only the absolute values are affected; the relative values and trends between the kernels under comparison remain consistent.




               * Test


Using the OPIMQ_EPOCH_PIN flag, we implemented the ability to enable or disable Epoch Pinning functionality in the OPIMQ kernel. The OPIMQ_EPOCH_PIN flag is defined in the source code at block/blk-mq.c.
(5_18_18_opext4_opimq)
We precompiled the kernel for both cases where OPIMQ_EPOCH_PIN is enabled and disabled. The kernel names are as follows:
               * Enabled: 5.18.18-opext4+opimq
               * Disabled: 5.18.18-opext4+opimq+nopin
We run the workload on three different kernels and compare their throughput:
               1. Baseline (5.18.18+original)
               2. Epoch Pin enabled (5.18.18-opext4+opimq)
               3. Epoch Pin disabled (5.18.18-opext4+opimq+nopin)
For both OPIMQ and OPIMQ (nopin), a 1.05% performance penalty is applied to simulate the penalty introduced by OPFTL (by multiplying the result by 0.9895). This ensures that the evaluation accounts for the overhead associated with order-preserving me chanisms in the FTL (Section 7 in paper).
               1. Please repeat the following process for each kernel;                 
uname -r      # Check the current kernel.
cd ./EPOCH_PIN
# 1. Filebench Varmail
./create_varmail_containers.sh 
./start_varmail_cont.sh 
Enter the experiment environment: Baseline (B) | OPIMQ w/ Epoch Pin (E) | OPIMQ NO Epoch Pin (NE)




# 2. Dbebch
./create_dbench_containers.sh 
./start_dbench_cont.sh
Enter the experiment environment: Baseline (B) | OPIMQ w/ Epoch Pin (E) | OPIMQ NO Epoch Pin (NE)


# 3. Sysbench OLTP insert for 120 seconds
./create_sysbench_containers.sh 
./start_sysbench_cont.sh 
Enter the experiment environment: Baseline (B) | OPIMQ w/ Epoch Pin (E) | OPIMQ NO Epoch Pin (NE)
	

               * ./create_${benchmark}_containers.sh loads the Docker image with the installed benchmark, creates 40 Docker containers, and copies the workload files run_${benchmark}_workload.sh required for running the benchmark into each container.  In the case of sysbench, 41 containers are created: one container acts as the MySQL server, and the remaining 40 containers serve as client containers.
               * ./start_${benchmark}_cont.sh checks if the workload scripts in the created Docker containers are running. Once all 40 containers are confirmed to be running, it simultaneously sends a SIGNAL. Upon receiving the SIGNAL, the containers execute the workload, ensuring that all 40 containers start the workload simultaneously.


               2. Parsing the Workload Results and Obtaining Throughput
               * All the experiment results are located in ./results. Under the results/ directory, a separate directory is created for each experimental environment. baseline opimq+nopin opimq+pin 
  



               * For each experimental environment, a directory is created for the corresponding benchmark.
  

               * The raw data file for each workload execution follows the format ${Concurrent Container-number}-${container id}. For instance, 40-1 indicates the result when 40 containers execute workloads simultaneously, this would be the workload result file for the container with ID 1.
                                  


               1. ./parse_varmail_result.sh : Parse the ops/sec values from the varmail raw data files, sum up the results from all 40 containers, and convert the total to Kops/sec.


Expected Output
  



               2. ./parse_dbench_result.sh : Sum the number of operations from the dbench raw data files of each container, then divide the total by the execution time (60 seconds) to calculate ops/sec for each container. Finally, sum up the ops/sec values from all containers and convert the total to Kops/sec.


Expected Output
          


               3. ./parse_sysbench_result.sh: Extract the transactions per second (Tx/Sec) values from the Sysbench raw data files of each container, sum them up across all containers, and convert the total to KTx/sec.
                                Expected Output
                                  


               3. Result Analysis: When the number of client containers is set to match the number of cores on the server (40 cores) to apply load, epoch pinning has minimal impact on throughput. Across all benchmarks—Filebench varmail, dbench, and sysbench—the performance difference between OPIMQ with epoch pinning and OPIMQ without epoch pinning is less than 1%. Epoch pinning is an essential feature to ensure intra-stream order dependency, and the benchmark results demonstrate that its overhead is negligible.
 
               * Implementation Details of Epoch Pinning.


  

Epoch pinning is implemented from lines 578 to 607 in block/blk-mq.c.
If OPIMQ_EPOCH_PIN is not defined, the original baseline source code is executed.
When OPIMQ_EPOCH_PIN is defined, it operates as described in Section 4.5 of the paper.


The CRQ in the paper is implemented using the queue_id of current, which represents the structure of the current kernel thread. This value is initialized to -1 when the thread is created.


In the original code, the software queue and hardware queue mapped to the currently running CPU ID are designated to register the request currently being processed.
When current->queue_id != current_cpu_id, epoch pinning is required.
In this case, instead of the queue corresponding to the current CPU ID, the request queue corresponding to current->queue_id is designated as the queue to register the current request.


The blk_mq_map_queue function specifies the hardware queue (NVMe command queue) mapped to the software queue (request queue).




               3. Container Throughput 
               * Overview
We evaluate OPIMQ's effectiveness in preserving storage order in a container-based cloud environment. We demonstrate the efficiency of OPIMQ on multi-queue devices through container scalability experiments. Modern cloud systems run services in containers for isolation, but shared storage leads to fsync()-induced serialization of all I/Os due to disk buffer flushing. OPIMQ replaces disk buffer flushing with the cache barrier command, allowing server applications to maintain storage order without serialization overhead.


We evaluate the performance using three workloads—Filebench varmail, Dbench, and Sysbench OLTP-insert on MySQL—across three order-preserving environments and the baseline Linux kernel (Figure 10 in paper):
               * OPIMQ: OPEXT4 + OPIMQ  (5.18.18-opext4+opimq)
               * Baseline: EXT4 + Vanilla Linux Kernel (5.18.18+original) 
               * MQFS: MQFS + ccNVMe (4.18.20-MQFS)
               * BFS + SQ: BarrierFS + Barrier IO Stack with a single command queue (hardware queue) enabled (5.18.18-bfs+sq)
Containers are created, and workloads are executed concurrently across these containers. After the workloads finish, the throughput of all containers is aggregated to assess performance. Each container runs the workload with a single thread.
For the BFS + SQ environment, the nvme_probe() function in the BarrierFS kernel is modified to set the number of I/O command queues initialized by the device to one. Since BarrierFS is a single-queue order-preserving file system (refer to Section 3 of the paper), one way to use it in a multi-queue setup is to enable only a single I/O queue.


               * Test : 


Filebench Varmail


Please execute the following to conduct the Filebench varmail experiment. By running the command below, you can obtain results corresponding to Figure 10(a). For the sake of experimental efficiency, we conducted the experiments only when the number of concurrently running containers was set to 40, 100, 200, 300, and 400.
uname -r         # 5.18.18-opext4+opimq / 5.18.18+original /4.18.20-MQFS / 5.18.18-bfs+sq
sudo su
cd ./CONTAINER_THROGHPUT
./run_varmail_test.sh
	

The entire experiment may take up to 30 minutes or longer.


Script Description: ./run_varmail_test.sh formats the device, mounts it to Docker's data directory (/mnt), and creates docker containers equal to the value set for N_CONT (400). It then copies the workload files (run_workload.sh and varmail.f) into each container. This script runs the varmail workload on containers simultaneously, with the number of active containers specified in test_list="40 100 200 300 400". If you wish to modify the number of containers running concurrently, update the test_list variable accordingly. This script executes a script named run_workload.sh for each container. The run_workload.sh script uses TRAP to make the containers wait for a SIGNAL. Upon receiving the SIGNAL, the containers simultaneously execute varmail.f. This file is a Filebench workload file that runs the varmail workload for 60 seconds.
        


After completing the experiment, please modify the kernel to ensure that the experiments can be performed for all configurations: MQFS, BFS w/SQ, OPIMQ, and Baseline. Then, repeat the experiments accordingly.


cd ../
./kernel
reboot
	                
Expected Output
(Please Note 1) We replaced the device used in the experiments due to the failure of the original one. The replacement device is the same model (Samsung 980 Pro NVMe SSD) but has a capacity of 1TB. As a result, the absolute throughput performance for Filebench varmail differs from what is reported in the paper. Varmail, being a workload that invokes fsync far more frequently than dbench or sysbench, explains the discrepancy in the results specifically for varmail. However, this affects only the absolute values; the relative values and trends between the compared kernels remain unchanged.


               * Path of the benchmark output
               * After executing the workload, the output raw data files are stored in ./results/${environment}/${benchmarks}. Similar to the Epoch Pinning experiments, the file format is ${Concurrent Container-number}-${container id}. For example, when executed with OPIMQ, the results are stored in the following path as follows:
cat ./results/OPIMQ/varmail
	        
          
For example, 100-1 represents the benchmark result file of the container with ID 1 when 100 containers were executed simultaneously.


               * Parsing the output 
By running the following script, you can obtain the throughput (KOPS/sec) benchmark results for each experimental environment. The script outputs the total ops/sec for all concurrently running containers as the result. For OPIMQ, an operational penalty of 1.05% is applied to the throughput to account for the OPFTL overhead. For BarrierFS, a 5% barrier write penalty is applied to the throughput as specified in the BarrierFS paper.


./parse_varmail.sh
	

Dbench


For dbench, since the saturation point is reached at 600 concurrently running containers, 600 containers are created in advance, and experiments are conducted where 40, 100, 200, 400, and 600 containers simultaneously execute the workload. Please execute the following to conduct the Filebench varmail experiment.  By running the command below, you can obtain results corresponding to Figure 10(b).
uname -r         # 5.18.18-opext4+opimq / 5.18.18+original /4.18.20-MQFS / 5.18.18-bfs+sq
sudo su
cd ./CONTAINER_THROGHPUT
./run_dbench_test.sh 
	The entire experiment may take up to 30 minutes or longer. 


The experiment method is identical to the approach used in the Filebench varmail experiments. ./run_dbench_test.sh script runs the dbench workload on containers simultaneously, with the number of active containers specified in test_list="40 100 200 400 600". If you wish to modify the number of containers running concurrently, update the test_list variable accordingly. 


After completing the experiment, please modify the kernel to ensure that the experiments can be performed for all configurations: MQFS, BFS w/SQ, OPIMQ, and Baseline. Then, repeat the experiments accordingly.


./kernel
reboot
	

Expected Output
               * After executing the workload, the output raw data files are stored in ./results/${environment}/dbench. The file format is ${Concurrent Container-number}-${container id}. For example, when executed with OPIMQ, the results are stored in the following path as follows:
cat ./results/OPIMQ/dbench
	               * Parsing the output: By running the following script, you can obtain the throughput (KOPS/sec) benchmark results for each experimental environment. 


./parse_dbench.sh
	

               * For OPIMQ, an operational penalty of 1.05% is applied to the throughput to account for the OPFTL overhead. For BarrierFS, a 5% barrier write penalty is applied to the throughput as specified in the BarrierFS paper.


Sysbench 
        
For sysbench, since saturation occurs at 400 containers, a total of 400 containers are created in advance, and experiments are conducted by varying the number of concurrently running containers. For efficient experimentation, we conduct experiments with 20, 40, 100, 200, and 400 containers running workloads concurrently. If you want to conduct experiments with different container count configurations and verify additional results, please modify the test_list values in the run_sysbench_test.sh script.




By running the command below, you can obtain results corresponding to Figure 10(b).
uname -r         # 5.18.18-opext4+opimq / 5.18.18+original /4.18.20-MQFS / 5.18.18-bfs+sq
sudo su
cd ./CONTAINER_THROGHPUT
./create_sysbench_containers.sh
//After all containers have been created and initialized.
./start_sysbench.sh
	The entire experiment may take up to 45 minutes or longer. 
The experiment method is identical to the approach used in the Filebench varmail and dbench experiments. For sysbench, a single MySQL server container is created, and the generated containers are connected to the MySQL server. Before each experiment, all tables in the MySQL database are deleted, new tables are created, and the oltp-insert workload is executed.


After completing the experiment, please modify the kernel to ensure that the experiments can be performed for all configurations: MQFS, BFS w/SQ, OPIMQ, and Baseline. Then, repeat the experiments accordingly.


./kernel
reboot
	

Expected Output
               * After executing the workload, the output raw data files are stored in ./results/${environment}/sysbench. The file format is ${Concurrent Container-number}-${container id}.


               * Parsing the output: By running the following script, you can obtain the throughput (KOPS/sec) benchmark results for each experimental environment.


./parse_sysbench.sh
	

               * For OPIMQ, an operational penalty of 1.05% is applied to the throughput to account for the OPFTL overhead. For BarrierFS, a 5% barrier write penalty is applied to the throughput as specified in the BarrierFS paper.






Result Analysis:  With OPIMQ, the docker container scalability outperforms the vanilla Linux IO stack
by 2.9×, 2.8×, and 2.9× under Filebench varmail, dbench, and sysbench, respectively.
In Filebench varmail workload, with 40 containers, OPIMQ outperforms EXT4, BFS-SQ and MQFS by 1.1 ×, 3.0 ×, 5.2 ×, respectively. OPIMQ shows 1.3 ×, 1.8 × and 6.4 × performance against BFS-SQ, EXT4, and MQFS when the number of containers is 400.


BarrierFS is designed based on a single queue, which leads to stream bounce and makes it unable to guarantee order when inter-queue dependencies occur. Therefore, to utilize BarrierFS in a multi-queue environment, it must be limited to using only a single queue. In contrast, OPIMQ can fully utilize all queues in a multi-queue environment while ensuring optimized low latency for fsync.


Note on OPFTL Experiments
The OPFTL source code is provided in the GitHub repository along with detailed implementation descriptions to facilitate a deeper understanding of its design and functionality. However, the OPFTL experiments require the specialized Cosmos board OpenSSD hardware, which is not readily accessible in typical environments. We kindly ask for your understanding regarding the omission of these experiments from the provided artifact. Instead, we reflect the overhead of OPFTL by applying a performance penalty in experiments conducted on the commercial device. For more detailed insights into the performance and design of OPFTL, please refer to the accompanying paper and the source code included in the artifact package.


Troubleshooting
Potential Issues
               * Server Connection Issues: Please ensure that the provided server address and credentials are correct.
               * Benchmark Failures (Experiments w/ Containers): If the total running containers keep printing in an infinite loop during the container experiment, press Ctrl + C and restart the script. If the issue persists, please reboot the server.
               * Trace Logs Missing: Please check that the tracing module is enabled and permissions are set correctly.
Contact
For further assistance, contact the author at: [pipiato@kaist.ac.kr]
